{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7647115-cb91-4a2c-b9c7-281d2b6f40f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.0.3 in ./venv/lib/python3.12/site-packages (2.0.3)\n",
      "Requirement already satisfied: Sastrawi==1.0.1 in ./venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: nltk==3.8.1 in ./venv/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm==4.66.4 in ./venv/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas==2.0.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas==2.0.3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.12/site-packages (from pandas==2.0.3) (2024.1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk==3.8.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk==3.8.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.12/site-packages (from nltk==3.8.1) (2024.5.10)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==2.0.3 Sastrawi==1.0.1 nltk==3.8.1 numpy==1.26.4 tqdm==4.66.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84753b75-6d7f-445b-be58-7de4f9e87c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, json, os, re, string, time\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67754874-a6cc-4b80-81fe-e31886a4ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yusufs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yusufs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a988508b-aa8d-4931-b98c-132e3375f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/42328068\n",
    "# write log into file, so we can trace it on which document it already processed\n",
    "\n",
    "log_filename = f\"{os.getcwd()}/progress.log\"\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                    format='%(levelname)s - %(asctime)s - %(name)s - %(message)s',\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO)\n",
    "file_handler = logging.FileHandler(log_filename)\n",
    "formatter = logging.Formatter('%(asctime)s : %(levelname)s : %(name)s : %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35b9f7c-64df-4a7b-b7a4-9c7a34b9c0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_author</th>\n",
       "      <th>title</th>\n",
       "      <th>portal</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>editor</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Infografis Pekerja Asing Dilarang Masuk Wilaya...</td>\n",
       "      <td>Liputan6.com</td>\n",
       "      <td>24 Jul 2021, 09:02 WIB</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Pemerintah melalui Menteri Hukum dan Hak Asasi...</td>\n",
       "      <td>https://www.liputan6.com/news/read/4614451/inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Infografis Jadwal Bulu Tangkis Indonesia di Ol...</td>\n",
       "      <td>Liputan6.com</td>\n",
       "      <td>23 Jul 2021, 23:23 WIB</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Bulu Tangkis menjadi andalan Indonesia berburu...</td>\n",
       "      <td>https://www.liputan6.com/bola/read/4614427/inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Infografis Jangan Bebal, Kamu Tidak Kebal Covi...</td>\n",
       "      <td>Liputan6.com</td>\n",
       "      <td>23 Jul 2021, 10:40 WIB</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Covid-19 tidak mengenal usia dan status. Siapa...</td>\n",
       "      <td>https://www.liputan6.com/news/read/4613233/inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Infografis Awas Perokok Lebih Rentan Tertular ...</td>\n",
       "      <td>Liputan6.com</td>\n",
       "      <td>22 Jul 2021, 10:35 WIB</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Kebiasaan merokok berisiko menimbulkan sejumla...</td>\n",
       "      <td>https://www.liputan6.com/news/read/4612324/inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Infografis Perbedaan Aturan PPKM Level 3 dan 4</td>\n",
       "      <td>Liputan6.com</td>\n",
       "      <td>22 Jul 2021, 09:01 WIB</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Abdillah</td>\n",
       "      <td>Pemberlakuan Pembatasan Kegiatan Masyarakat at...</td>\n",
       "      <td>https://www.liputan6.com/news/read/4612511/inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  id_author                                              title  \\\n",
       "0   0          1  Infografis Pekerja Asing Dilarang Masuk Wilaya...   \n",
       "1   1          1  Infografis Jadwal Bulu Tangkis Indonesia di Ol...   \n",
       "2   2          1  Infografis Jangan Bebal, Kamu Tidak Kebal Covi...   \n",
       "3   3          1  Infografis Awas Perokok Lebih Rentan Tertular ...   \n",
       "4   4          1     Infografis Perbedaan Aturan PPKM Level 3 dan 4   \n",
       "\n",
       "         portal                    time    author    editor  \\\n",
       "0  Liputan6.com  24 Jul 2021, 09:02 WIB  Abdillah  Abdillah   \n",
       "1  Liputan6.com  23 Jul 2021, 23:23 WIB  Abdillah  Abdillah   \n",
       "2  Liputan6.com  23 Jul 2021, 10:40 WIB  Abdillah  Abdillah   \n",
       "3  Liputan6.com  22 Jul 2021, 10:35 WIB  Abdillah  Abdillah   \n",
       "4  Liputan6.com  22 Jul 2021, 09:01 WIB  Abdillah  Abdillah   \n",
       "\n",
       "                                             content  \\\n",
       "0  Pemerintah melalui Menteri Hukum dan Hak Asasi...   \n",
       "1  Bulu Tangkis menjadi andalan Indonesia berburu...   \n",
       "2  Covid-19 tidak mengenal usia dan status. Siapa...   \n",
       "3  Kebiasaan merokok berisiko menimbulkan sejumla...   \n",
       "4  Pemberlakuan Pembatasan Kegiatan Masyarakat at...   \n",
       "\n",
       "                                              source  \n",
       "0  https://www.liputan6.com/news/read/4614451/inf...  \n",
       "1  https://www.liputan6.com/bola/read/4614427/inf...  \n",
       "2  https://www.liputan6.com/news/read/4613233/inf...  \n",
       "3  https://www.liputan6.com/news/read/4612324/inf...  \n",
       "4  https://www.liputan6.com/news/read/4612511/inf...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{os.getcwd()}/news.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e2e553-9fb5-4b9a-8857-a90202232561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains 14343 rows of documents with data type <class 'pandas.core.frame.DataFrame'>.\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows\n",
    "print(f\"Contains {len(df)} rows of documents with data type {type(df)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f31668-d445-460f-bea4-8caf843e413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tweet_special(text: str) -> str:\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub('([@#][A-Za-z0-9]+)|(\\\\w+:\\\\S+)',\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "#remove number\n",
    "def remove_number(text: str) -> str:\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text: str) -> str:\n",
    "    return re.sub('\\\\s+',' ',text)\n",
    "\n",
    "# remove single char\n",
    "def remove_single_char(text: str) -> str:\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fc3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_doc_freq(data: dict[str, dict[str, int]]) -> dict[str, dict[str, List[dict[str, int]]]]:\n",
    "    # convert: {\"pemerintah\": {\"0\": 1},\"dan\": {\"1\": 1, \"0\": 3}}\n",
    "    # into   : {\"pemerintah\": [{\"0\": 1}],\"dan\": [{\"0\": 3}, {\"1\": 1}]}\n",
    "    output_data = {}\n",
    "    for key, value in data.items():\n",
    "        temp_dict = []\n",
    "        for k, v in value.items():\n",
    "            temp_dict.append({k: v})\n",
    "        output_data[key] = sorted(temp_dict, key=lambda x: list(x.values())[0], reverse=True)\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9eb11a7-458b-4be4-99cd-c7383654acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_sequential(data: pd.core.frame.DataFrame) -> dict:\n",
    "  \"\"\"\n",
    "  Creates an inverted index for a 'id' and 'content' column in a pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the id and content text data.\n",
    "\n",
    "  Returns:\n",
    "      * A dictionary representing the inverted index.\n",
    "      * A dictionary containing the document content.\n",
    "  \"\"\"\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  # create stemmer\n",
    "  factory = StemmerFactory()\n",
    "  stemmer = factory.create_stemmer()\n",
    "\n",
    "  # get stopword indonesia\n",
    "  list_stopwords = stopwords.words('indonesian')\n",
    "  list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
    "                         'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
    "                         'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                         'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                         'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                         'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                         '&amp', 'yah'])\n",
    "\n",
    "  # convert list to dictionary\n",
    "  list_stopwords = set(list_stopwords)\n",
    "\n",
    "  document_content_map = {}\n",
    "  word_map_not_stemmed_all_word = {}\n",
    "  word_map_stemmed_not_stopword = {}\n",
    "  word_map_stemmed_all_word = {}\n",
    "  elapsed_time_total = 0\n",
    "  for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    start_per_doc = time.time()\n",
    "    count = index + 1  \n",
    "\n",
    "    id = int(row['id'])\n",
    "    content = row['content']\n",
    "\n",
    "    document_content_map[id] = content\n",
    "    text = str(content).lower().split()  # Case folding and split words\n",
    "    for word in text:\n",
    "      word = word.strip() # remove prefix and suffix space\n",
    "\n",
    "      # tokenizing\n",
    "      logger.info(f\"{((count/len(data))/100):.2f}% Document {id} tokenizing word: {word}\")\n",
    "      word = remove_tweet_special(text=word)\n",
    "      word = remove_number(text=word)\n",
    "      word = remove_punctuation(text=word)\n",
    "      word = remove_whitespace_multiple(text=word)\n",
    "      word = remove_single_char(text=word)\n",
    "      word = word.strip()\n",
    "      logger.info(f\"{((count/len(data))/100):.2f}% Document {id} finish tokenizing word: {word}\")\n",
    "\n",
    "      if word == \"\":\n",
    "          continue\n",
    "\n",
    "      # save not stemming, unchecked whether it is in stopwords of not\n",
    "      id_map_not_stem_all_word = {}\n",
    "      if word in word_map_not_stemmed_all_word:\n",
    "        id_map_not_stem_all_word = word_map_not_stemmed_all_word[word]\n",
    "      \n",
    "      if id in id_map_not_stem_all_word:\n",
    "        id_map_not_stem_all_word[id] = id_map_not_stem_all_word[id] + 1\n",
    "      else:\n",
    "        id_map_not_stem_all_word[id] = 1\n",
    "\n",
    "      word_map_not_stemmed_all_word[word] = id_map_not_stem_all_word\n",
    "\n",
    "      logger.info(f\"{((count/len(data))/100):.2f}% Document {id} stemming word: {word}\")\n",
    "      stemmed_word = stemmer.stem(word)\n",
    "      logger.info(f\"{((count/len(data))/100):.2f}% Document {id} finish stemming word: {word} -> {stemmed_word}\")\n",
    "      word = stemmed_word\n",
    "        \n",
    "      # save stemming + not in stop words\n",
    "      if word not in list_stopwords:\n",
    "        id_map = {}\n",
    "        if word in word_map_stemmed_not_stopword:\n",
    "          id_map = word_map_stemmed_not_stopword[word]\n",
    "\n",
    "        if id in id_map:\n",
    "          id_map[id] = id_map[id] + 1\n",
    "        else:\n",
    "          id_map[id] = 1\n",
    "\n",
    "        word_map_stemmed_not_stopword[word] = id_map\n",
    "\n",
    "      # save all stemmed words, even if it contains stop words\n",
    "      id_map_stemmed_all = {}\n",
    "      if word in word_map_stemmed_all_word:\n",
    "        id_map_stemmed_all = word_map_stemmed_all_word[word]\n",
    "\n",
    "      if id in id_map_stemmed_all:\n",
    "        id_map_stemmed_all[id] = id_map_stemmed_all[id] + 1\n",
    "      else:\n",
    "        id_map_stemmed_all[id] = 1\n",
    "\n",
    "      word_map_stemmed_all_word[word] = id_map_stemmed_all\n",
    "\n",
    "    end_per_doc = time.time()\n",
    "    elapsed_time_total = elapsed_time_total + (end_per_doc - start_per_doc)\n",
    "\n",
    "  word_map_not_stemmed_all_word = sort_by_doc_freq(word_map_not_stemmed_all_word)\n",
    "  word_map_stemmed_not_stopword = sort_by_doc_freq(word_map_stemmed_not_stopword)\n",
    "  word_map_stemmed_all_word = sort_by_doc_freq(word_map_stemmed_all_word)\n",
    "\n",
    "  end = time.time()\n",
    "  elapsed_time = end - start\n",
    "  print(f\"Total time to index: {elapsed_time:.6f} seconds, avg: {(elapsed_time/len(document_content_map)):.6f} seconds/doc\")\n",
    "  return {\n",
    "      'document_content_map': document_content_map,\n",
    "      'word_map_not_stemmed_all_word': word_map_not_stemmed_all_word, \n",
    "      'word_map_stemmed_not_stopword': word_map_stemmed_not_stopword, \n",
    "      'word_map_stemmed_all_word': word_map_stemmed_all_word,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c942530-ec41-42d4-9ed6-316baa19be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14343/14343 [1:35:58<00:00,  2.49it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to index: 5761.201422 seconds, avg: 0.401673 seconds/doc\n",
      "Success indexing 14343 documents\n",
      "Success write /Users/yusufs/Magister-Ilmu-Komputer/materi-kuliah/semester-3/information-retrieval/20240504-boolean-retrieval/document_content_map.json (14343)\n",
      "Success write /Users/yusufs/Magister-Ilmu-Komputer/materi-kuliah/semester-3/information-retrieval/20240504-boolean-retrieval/word_map_not_stemmed_all_word.json (92778)\n",
      "Success write /Users/yusufs/Magister-Ilmu-Komputer/materi-kuliah/semester-3/information-retrieval/20240504-boolean-retrieval/word_map_stemmed_not_stopword.json (73256)\n",
      "Success write /Users/yusufs/Magister-Ilmu-Komputer/materi-kuliah/semester-3/information-retrieval/20240504-boolean-retrieval/word_map_stemmed_all_word.json (73573)\n"
     ]
    }
   ],
   "source": [
    "# Do process\n",
    "result = inverted_index_sequential(data=df)\n",
    "\n",
    "document_content_map = result['document_content_map']\n",
    "word_map_not_stemmed_all_word = result['word_map_not_stemmed_all_word']\n",
    "word_map_stemmed_not_stopword = result['word_map_stemmed_not_stopword']\n",
    "word_map_stemmed_all_word = result['word_map_stemmed_all_word']\n",
    "\n",
    "\n",
    "print(f\"Success indexing {len(document_content_map)} documents\")\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "with open(f\"{base_dir}/document_content_map.json\", \"w\") as f:\n",
    "  json_string = json.dumps(document_content_map)\n",
    "  f.write(json_string)\n",
    "  print(f\"Success write {base_dir}/document_content_map.json ({len(document_content_map)})\")\n",
    "\n",
    "with open(f\"{base_dir}/word_map_not_stemmed_all_word.json\", \"w\") as f:\n",
    "  json_string = json.dumps(word_map_not_stemmed_all_word)\n",
    "  f.write(json_string)\n",
    "  print(f\"Success write {base_dir}/word_map_not_stemmed_all_word.json ({len(word_map_not_stemmed_all_word)})\")\n",
    "\n",
    "with open(f\"{base_dir}/word_map_stemmed_not_stopword.json\", \"w\") as f:\n",
    "  json_string = json.dumps(word_map_stemmed_not_stopword)\n",
    "  f.write(json_string)\n",
    "  print(f\"Success write {base_dir}/word_map_stemmed_not_stopword.json ({len(word_map_stemmed_not_stopword)})\")\n",
    "\n",
    "with open(f\"{base_dir}/word_map_stemmed_all_word.json\", \"w\") as f:\n",
    "  json_string = json.dumps(word_map_stemmed_all_word)\n",
    "  f.write(json_string)\n",
    "  print(f\"Success write {base_dir}/word_map_stemmed_all_word.json ({len(word_map_stemmed_all_word)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
